{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model outputs.\n",
    "        labels (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Accuracy of the model.\n",
    "    \"\"\"\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def training_step(model, batch):\n",
    "    \"\"\"\n",
    "    Perform a single training step.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        batch (tuple): A tuple containing batch of input images and labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Loss of the model on the batch.\n",
    "    \"\"\"\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    out = model(images)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch):\n",
    "    \"\"\"\n",
    "    Perform a single validation step.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        batch (tuple): A tuple containing batch of input images and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the loss and accuracy of the model on the batch.\n",
    "    \"\"\"\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    out = model(images)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    acc = accuracy(out, labels)\n",
    "    return {'Loss': loss.detach(), 'Acc': acc}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    \"\"\"\n",
    "    Calculate the average loss and accuracy over all batches in a validation epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        outputs (list): List of dictionaries containing the loss and accuracy of each validation batch.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the average loss and accuracy over all batches.\n",
    "    \"\"\"\n",
    "    batch_losses = [x['Loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()\n",
    "    batch_accs = [x['Acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()\n",
    "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(model, epoch, result):\n",
    "    \"\"\"\n",
    "    Print the epoch summary.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        epoch (int): Current epoch number.\n",
    "        result (dict): Dictionary containing the training and validation metrics for the epoch.\n",
    "    \"\"\"\n",
    "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "        epoch, result['lrs'][-1], result['train_loss'], result['Loss'], result['Acc']))\n",
    "\n",
    "def distance(model, model0):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two models.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The first neural network model.\n",
    "        model0 (torch.nn.Module): The second neural network model.\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized distance between the two models.\n",
    "    \"\"\"\n",
    "    distance = 0\n",
    "    normalization = 0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space = '  ' if 'bias' in k else ''\n",
    "        current_dist = (p.data - p0.data).pow(2).sum().item()\n",
    "        current_norm = p.data.pow(2).sum().item()\n",
    "        distance += current_dist\n",
    "        normalization += current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0 * np.sqrt(distance / normalization)}')\n",
    "    return 1.0 * np.sqrt(distance / normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the average loss and accuracy over the validation dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
    "    return validation_epoch_end(model, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Learning Rate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    \"\"\"\n",
    "    Get the current learning rate of the optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "\n",
    "    Returns:\n",
    "        float: The current learning rate.\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit One Cycle Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    Train the model using the One Cycle Policy.\n",
    "\n",
    "    Args:\n",
    "        epochs (int): Number of epochs to train.\n",
    "        max_lr (float): Maximum learning rate.\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "        weight_decay (float): Weight decay (L2 regularization) factor (default: 0).\n",
    "        grad_clip (float or None): Gradient clipping value (default: None).\n",
    "        opt_func (torch.optim.Optimizer): The optimizer class (default: torch.optim.SGD).\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing training and validation metrics for each epoch.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(model, batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lrs.append(get_lr(optimizer))\n",
    "\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Loading Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload the dataset\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url, '.')\n",
    "\n",
    "# Extract from archive\n",
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')\n",
    "\n",
    "# Look into the data directory\n",
    "data_dir = './data/cifar10'\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)\n",
    "\n",
    "\"\"\"\n",
    "Loading the CIFAR-10 dataset with a collection of 60,000 32x32 color images in 10 classes, with 6,000 images per class. \n",
    "The classes are: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', and 'truck'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for training images\n",
    "transform_train = tt.Compose([\n",
    "    tt.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    # Normalize the image with the given mean and standard deviation\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Define the transformation for testing images\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    # Normalize the image with the given mean and standard deviation\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset using ImageFolder with the specified transformations\n",
    "train_ds = ImageFolder(data_dir+'/train', transform_train)\n",
    "\n",
    "# Create validation dataset using ImageFolder with the specified transformations\n",
    "valid_ds = ImageFolder(data_dir+'/test', transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Create training data loader with the specified batch size, shuffling, and other parameters\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "\n",
    "# Create validation data loader with the specified batch size, shuffling, and other parameters\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
