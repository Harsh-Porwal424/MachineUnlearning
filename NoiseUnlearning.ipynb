{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model outputs.\n",
    "        labels (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Accuracy of the model.\n",
    "    \"\"\"\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def training_step(model, batch):\n",
    "    \"\"\"\n",
    "    Perform a single training step.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        batch (tuple): A tuple containing batch of input images and labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Loss of the model on the batch.\n",
    "    \"\"\"\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    out = model(images)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch):\n",
    "    \"\"\"\n",
    "    Perform a single validation step.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        batch (tuple): A tuple containing batch of input images and labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the loss and accuracy of the model on the batch.\n",
    "    \"\"\"\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    out = model(images)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    acc = accuracy(out, labels)\n",
    "    return {'Loss': loss.detach(), 'Acc': acc}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    \"\"\"\n",
    "    Calculate the average loss and accuracy over all batches in a validation epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        outputs (list): List of dictionaries containing the loss and accuracy of each validation batch.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the average loss and accuracy over all batches.\n",
    "    \"\"\"\n",
    "    batch_losses = [x['Loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()\n",
    "    batch_accs = [x['Acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()\n",
    "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(model, epoch, result):\n",
    "    \"\"\"\n",
    "    Print the epoch summary.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        epoch (int): Current epoch number.\n",
    "        result (dict): Dictionary containing the training and validation metrics for the epoch.\n",
    "    \"\"\"\n",
    "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "        epoch, result['lrs'][-1], result['train_loss'], result['Loss'], result['Acc']))\n",
    "\n",
    "def distance(model, model0):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two models.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The first neural network model.\n",
    "        model0 (torch.nn.Module): The second neural network model.\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized distance between the two models.\n",
    "    \"\"\"\n",
    "    distance = 0\n",
    "    normalization = 0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space = '  ' if 'bias' in k else ''\n",
    "        current_dist = (p.data - p0.data).pow(2).sum().item()\n",
    "        current_norm = p.data.pow(2).sum().item()\n",
    "        distance += current_dist\n",
    "        normalization += current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0 * np.sqrt(distance / normalization)}')\n",
    "    return 1.0 * np.sqrt(distance / normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the average loss and accuracy over the validation dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
    "    return validation_epoch_end(model, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Learning Rate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    \"\"\"\n",
    "    Get the current learning rate of the optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "\n",
    "    Returns:\n",
    "        float: The current learning rate.\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit One Cycle Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    Train the model using the One Cycle Policy.\n",
    "\n",
    "    Args:\n",
    "        epochs (int): Number of epochs to train.\n",
    "        max_lr (float): Maximum learning rate.\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "        weight_decay (float): Weight decay (L2 regularization) factor (default: 0).\n",
    "        grad_clip (float or None): Gradient clipping value (default: None).\n",
    "        opt_func (torch.optim.Optimizer): The optimizer class (default: torch.optim.SGD).\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing training and validation metrics for each epoch.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(model, batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lrs.append(get_lr(optimizer))\n",
    "\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "    return history\n",
    "\n",
    "\"\"\"\n",
    "Implements the one-cycle learning rate policy for training a neural network.\n",
    "It trains the model for the specified number of epochs using the train_loader for training and \n",
    "the val_loader for validation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Loading Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload the dataset\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url, '.')\n",
    "\n",
    "# Extract from archive\n",
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')\n",
    "\n",
    "# Look into the data directory\n",
    "data_dir = './data/cifar10'\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)\n",
    "\n",
    "\"\"\"\n",
    "Loading the CIFAR-10 dataset with a collection of 60,000 32x32 color images in 10 classes, with 6,000 images per class. \n",
    "The classes are: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', and 'truck'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for training images\n",
    "transform_train = tt.Compose([\n",
    "    tt.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    # Normalize the image with the given mean and standard deviation\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Define the transformation for testing images\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    # Normalize the image with the given mean and standard deviation\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset using ImageFolder with the specified transformations\n",
    "train_ds = ImageFolder(data_dir+'/train', transform_train)\n",
    "\n",
    "# Create validation dataset using ImageFolder with the specified transformations\n",
    "valid_ds = ImageFolder(data_dir+'/test', transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Create training data loader with the specified batch size, shuffling, and other parameters\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "\n",
    "# Create validation data loader with the specified batch size, shuffling, and other parameters\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the device for training (GPU if available)\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Create the ResNet-18 model for image classification with 10 classes and move it to the specified device\n",
    "model = resnet18(num_classes=10).to(device=device)\n",
    "\n",
    "# Define the number of training epochs\n",
    "epochs = 40\n",
    "\n",
    "# Set the maximum learning rate for the One Cycle Policy\n",
    "max_lr = 0.01\n",
    "\n",
    "# Set the gradient clipping threshold\n",
    "grad_clip = 0.1\n",
    "\n",
    "# Set the weight decay for regularization\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Choose the optimizer for training the model\n",
    "opt_func = torch.optim.Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model using the fit_one_cycle function and measure the time taken\n",
    "history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl,\n",
    "                         grad_clip=grad_clip,\n",
    "                         weight_decay=weight_decay,\n",
    "                         opt_func=opt_func)\n",
    "\n",
    "# Save the trained model's state dictionary to a file\n",
    "torch.save(model.state_dict(), \"ResNET18_CIFAR10_ALL_CLASSES.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model's state dictionary from the file\n",
    "model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n",
    "\n",
    "# Evaluate the model on the validation dataset and store the results in history\n",
    "history = [evaluate(model, valid_dl)]\n",
    "history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for adding noise to the input data\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self, *dim):\n",
    "        super().__init__()\n",
    "        # Initialize a noise tensor as a learnable parameter\n",
    "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad=True)\n",
    "\n",
    "    def forward(self):\n",
    "        # Return the noise tensor\n",
    "        return self.noise\n",
    "\n",
    "\"\"\"\n",
    "Noise Module that represents a learnable noise tensor.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all classes\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# classes which are required to un-learn\n",
    "classes_to_forget = [0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store class-wise samples for training and validation\n",
    "num_classes = 10\n",
    "classwise_train = {}\n",
    "classwise_test = {}\n",
    "\n",
    "# Populate class-wise dictionaries for training dataset\n",
    "for i in range(num_classes):\n",
    "    classwise_train[i] = []\n",
    "\n",
    "# Populate class-wise dictionaries for validation dataset\n",
    "for i in range(num_classes):\n",
    "    classwise_test[i] = []\n",
    "\n",
    "# Iterate over training dataset and populate class-wise dictionaries\n",
    "for img, label in train_ds:\n",
    "    classwise_train[label].append((img, label))\n",
    "\n",
    "# Iterate over validation dataset and populate class-wise dictionaries\n",
    "for img, label in valid_ds:\n",
    "    classwise_test[label].append((img, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some samples from classes that are not in the forget list\n",
    "retain_samples = []\n",
    "for cls in range(num_classes):\n",
    "    if cls not in classes_to_forget:\n",
    "        retain_samples.extend(classwise_train[cls][:num_samples_per_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain validation set\n",
    "retain_valid = [(img, label) for cls in range(num_classes) if cls not in classes_to_forget\n",
    "                for img, label in classwise_test[cls]]\n",
    "\n",
    "# Forget validation set\n",
    "forget_valid = [(img, label) for cls in range(num_classes) if cls in classes_to_forget\n",
    "                for img, label in classwise_test[cls]]\n",
    "\n",
    "# Create data loaders for forget and retain validation sets\n",
    "forget_valid_dl = DataLoader(forget_valid, batch_size, num_workers=3, pin_memory=True)\n",
    "retain_valid_dl = DataLoader(retain_valid, batch_size*2, num_workers=3, pin_memory=True)\n",
    "\n",
    "\"\"\"\n",
    "Now we have two DataLoader objects (forget_valid_dl and retain_valid_dl) \n",
    "that can be used to iterate over the validation set for the classes \n",
    "to forget and the classes to retain, respectively.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet-18 model with the specified number of classes and move it to the device\n",
    "model = resnet18(num_classes=10).to(device=device)\n",
    "\n",
    "# Load the trained model's state dictionary from the file\n",
    "model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a dictionary to store noise tensors for each class\n",
    "noises = {}\n",
    "\n",
    "# Optimize the loss for each class in classes_to_forget\n",
    "for cls in classes_to_forget:\n",
    "    print(\"Optimizing loss for class {}\".format(cls))\n",
    "    # Initialize a Noise module for the current class\n",
    "    noises[cls] = Noise(batch_size, 3, 32, 32).cuda()\n",
    "    # Use Adam optimizer for the noise parameters\n",
    "    opt = torch.optim.Adam(noises[cls].parameters(), lr=0.1)\n",
    "\n",
    "    # Number of epochs and steps for optimization\n",
    "    num_epochs = 5\n",
    "    num_steps = 8\n",
    "    class_label = cls\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = []\n",
    "        # Iterate over steps within each epoch\n",
    "        for batch in range(num_steps):\n",
    "            inputs = noises[cls]()  # Get the noise tensor\n",
    "            labels = torch.zeros(batch_size).cuda() + class_label  # Create labels for the current class\n",
    "            outputs = model(inputs)  # Get model outputs\n",
    "            # Calculate loss with a penalty on the noise\n",
    "            loss = -F.cross_entropy(outputs, labels.long()) + 0.1 * torch.mean(torch.sum(torch.square(inputs), [1, 2, 3]))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss.append(loss.cpu().detach().numpy())\n",
    "        print(\"Loss: {}\".format(np.mean(total_loss)))\n",
    "\n",
    "\"\"\"\n",
    "* Optimizing the noise tensors for the classes that need to be unlearned \n",
    "\n",
    "* This code uses the noise tensors to perturb the model's inputs, effectively \"unlearning\" \n",
    "the specified classes by modifying the inputs during inference. The regularization term\n",
    "helps prevent the noise from becoming too large, which could negatively impact the model's performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set batch size for the noisy data loader\n",
    "batch_size = 256\n",
    "# Initialize a list to store noisy data samples\n",
    "noisy_data = []\n",
    "# Number of batches for each class to add noise\n",
    "num_batches = 20\n",
    "# Class number for noisy samples\n",
    "class_num = 0\n",
    "\n",
    "# Add noisy samples for each class in classes_to_forget\n",
    "for cls in classes_to_forget:\n",
    "    for i in range(num_batches):\n",
    "        # Get noise tensor for the current class and detach it to CPU\n",
    "        batch = noises[cls]().cpu().detach()\n",
    "        # Add each image from the noise tensor as a noisy sample\n",
    "        for i in range(batch[0].size(0)):\n",
    "            noisy_data.append((batch[i], torch.tensor(class_num)))\n",
    "\n",
    "# Add other samples (retain samples) to the noisy data\n",
    "other_samples = [(sample[0].cpu(), torch.tensor(sample[1])) for sample in retain_samples]\n",
    "noisy_data += other_samples\n",
    "\n",
    "# Create a data loader for the noisy data\n",
    "noisy_loader = torch.utils.data.DataLoader(noisy_data, batch_size=256, shuffle=True)\n",
    "\n",
    "# Define optimizer for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0\n",
    "    for i, data in enumerate(noisy_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), torch.tensor(labels).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        out = torch.argmax(outputs.detach(), dim=1)\n",
    "        assert out.shape == labels.shape\n",
    "        running_acc += (labels == out).sum().item()\n",
    "\n",
    "    # Print statistics for the epoch\n",
    "    print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)}, Train Acc: {running_acc*100/len(train_ds)}%\")\n",
    "\n",
    "\"\"\"\n",
    "It trains the model on a dataset containing noisy samples from \n",
    "the classes that need to be unlearned (classes_to_forget) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "history = [evaluate(model, forget_valid_dl)]\n",
    "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "history = [evaluate(model, retain_valid_dl)]\n",
    "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create a data loader for the other samples (retain samples)\n",
    "heal_loader = torch.utils.data.DataLoader(other_samples, batch_size=256, shuffle=True)\n",
    "\n",
    "# Define optimizer for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0\n",
    "    for i, data in enumerate(heal_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), torch.tensor(labels).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        out = torch.argmax(outputs.detach(), dim=1)\n",
    "        assert out.shape == labels.shape\n",
    "        running_acc += (labels == out).sum().item()\n",
    "\n",
    "    # Print statistics for the epoch\n",
    "    print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)}, Train Acc: {running_acc*100/len(train_ds)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "history = [evaluate(model, forget_valid_dl)]\n",
    "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "history = [evaluate(model, retain_valid_dl)]\n",
    "print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
